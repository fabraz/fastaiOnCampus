{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Reconhecimento de flores e árvores\n\n## Autora\n* Isabella Carneiro Godinho de Morais Sá\n* Matrícula: 18/0019066\n* Github: isabellacgmsa\n\n## Objetivo\nO propósito do presente artigo é demonstrar o conhecimento adquirido através da terceira lição, que fala sobre diferentes modelos de treinamento.\n\n## Modelos\nAté o momento estava utilizando o Resnet, porém na terceira lição há a indagação de qual seria o melhor modelo tendo em vista as particularidades de cada projeto. Para chegar a uma conclusão, é necessário testá-los.\n\n## Passo 1 \nA instalação das dependências é um passo fundamental para garantir que o processo possa ser executado com sucesso e que os resultados sejam precisos e confiáveis. Além disso, a escolha das bibliotecas e ferramentas adequadas é essencial para garantir que o processo seja executado de maneira eficiente e eficaz, maximizando os recursos disponíveis e minimizando o tempo necessário para a execução do processo.\nAssim sendo, o primeiro passo que foi tomado para iniciar o projeto foi a instalação das dependências necessárias para que o processo pudesse ser executado de maneira adequada.\nPara tanto, foram utilizados os seguintes comandos para a instalação das dependências necessárias:\n\n```pip install -U duckduckgo_search```\n\n```pip install fastai\"```\n\nVale ressaltar que, antes de iniciar o processo, é necessário fazer a instalação do PyTorch, que é uma biblioteca de aprendizado de máquina de código aberto amplamente utilizada para tarefas como visão computacional e processamento de linguagem natural.\n\n## Passo 2 \nPara fazer a instalação das imagens foi utilizado o ddg_images, como mostra o código a seguir:  \n```\nfrom duckduckgo_search import ddg_images\nfrom fastcore.all import *\n\ndef search_images(term: str, max_images=30):\n    print(f\"Searching for '{term}'\")\n    return L(ddg_images(term, max_results=max_images)).itemgot('image')\n    \n```\n\nO primeiro teste será feito baixando apenas uma imagem de flor:\n\n```\nurls = search_images('flower', max_images=1)\nurls[2]\n```\n\nO download será realizado utilizando a biblioteca fastdownload:\n\n```\nfrom fastdownload import download_url\ndest = 'oneflower.jpg'\ndownload_url(urls[0], dest, show_progress=False)\n\nfrom fastai.vision.all import *\nim = Image.open(dest)\nim.to_thumb(256,256)\n```\n\n![texto](https://s3.static.brasilescola.uol.com.br/be/2020/12/girassol.jpg)\n\nAo constatar que está funcionando todo o processo é possível fazer o download de dados mais numerosos:\n```\nsearches = 'tree', 'flower'\npath = Path('tree_flower')\nfrom time import sleep\n\nfor o in searches:\n    dest = (path/o)\n    dest.mkdir(exist_ok=True, parents=True)\n    download_images(dest, urls=search_images(f'{o} tree'))\n    sleep(10)\n    download_images(dest, urls=search_images(f'{o} flower'))\n    sleep(10)\n    resize_images(path/o, max_size=400, dest= path/o)\n```\n\nFoi utilizado o sleep para que não haja problemas no tráfego de informações.\n\nEssas imagens serão salvas em uma pasta cujo caminho está definido no path.\n\nAntes de iniciar o treinamento do modelo de fato é necessário garantir que não haja imagens com erro para evitar futuros problemas:\n\n```\nfailed = verify_images(get_image_files(path))\nfailed.map(Path.unlink)\nlen(failed)\n```\n\nPara treinar modelos de aprendizado de máquina com conjuntos de dados volumosos, é necessário dividir os dados em lotes menores para evitar sobrecarregar a memória RAM do computador. O Dataloader é uma classe que oferece essa funcionalidade de forma eficiente. Ele carrega e processa os dados em lotes menores, tornando o processo de treinamento escalável.\n\nO fastai é um framework de aprendizado de máquina que permite o uso do datablock, uma ferramenta que facilita a personalização do processamento e organização dos dados em lotes para treinamento do modelo. Com o datablock, é possível definir etapas como leitura, pré-processamento e organização dos dados de forma mais simples. Isso permite que os usuários criem fluxos de trabalho personalizados e experimentem diferentes abordagens para melhorar a precisão do modelo.\n\n```\ndls = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=[Resize(192, method='squish')]\n).dataloaders(path, bs=32)\n\ndls.show_batch(max_n=6)\n```\n![texto](https://i.ibb.co/T4XzYk1/flores.png)\n\nFoi especificado que há o ImageBlock, que é uma classe do fastai que permite a criação de blocos para processar as imagens no modelo, e o CategoryBlock que cria blocos para processar variáveis categóricas, nesse caso árvore ou flor. Nesse trecho também especifica o tamanho que as imagens terão, já que precisa ser o mesmo para todas.\n\n### Treinamentos\n### ResNet18\nO ResNet18 é um modelo de rede neural convolucional que possui 18 camadas e é considerado uma versão mais compacta do ResNet original. Isso o torna mais fácil de ser treinado e executado em dispositivos com recursos limitados. Ele foi pré-treinado em grandes conjuntos de dados de imagens, como o ImageNet, o que permite sua utilização em tarefas de classificação de imagem com alta precisão.\n\nEm síntese, o ResNet18 utiliza blocos residuais para permitir que informações sejam transmitidas pelas camadas de forma mais eficiente, permitindo a construção de redes mais profundas. Devido à sua eficácia e eficiência, é uma escolha popular para tarefas de classificação de imagem.\n```\nlearn = vision_learner(dls, resnet18, metrics=error_rate).to_fp16()\nlearn.fine_tune(8)\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n```\n![imagem](https://i.ibb.co/j6VNmbC/matriz-Copia.png)\n\n\n### Outro\nA arquitetura do RegNetY é composta por um módulo de construção de bloco que é replicado várias vezes para formar a rede. Cada bloco é constituído por um conjunto de camadas, que incluem convoluções, normalização em lote e ativação, e possui uma conexão residual para permitir a passagem de informações pela rede de maneira mais eficiente. O número de blocos e a largura de banda são definidos por meio de uma pesquisa automática de arquitetura baseada na RegNetX, uma arquitetura anterior desenvolvida pela equipe do FAIR.\n\nEm comparação com outras arquiteturas populares, como ResNet e EfficientNet, o RegNetY é capaz de atingir resultados de precisão de ponta em conjuntos de dados de referência, como o ImageNet, utilizando uma arquitetura mais enxuta e eficiente.\n```\nlearn2 = vision_learner(dls, 'regnety_008', metrics=error_rate).to_fp16()\nlearn2.fine_tune(8)\n\ninterp = ClassificationInterpretation.from_learner(learn2)\ninterp.plot_confusion_matrix()\n```\n![imagem](https://i.ibb.co/PG2hMjz/matriz.png)\n\n\n\n### Outro\nA arquitetura LeViT consiste em um conjunto de blocos de convolução intercalados com blocos de atenção, uma característica comum da arquitetura Transformer. Após cada bloco de convolução, há um bloco de atenção para permitir que a rede capture informações espaciais e relacionais em diferentes níveis de abstração.\n\nO objetivo da LeViT é combinar o desempenho de arquiteturas baseadas em Transformer com a eficiência de modelos convolucionais, tornando-a uma opção adequada para aplicações em tempo real em dispositivos com recursos limitados. De acordo com a equipe do FAIR, a LeViT alcançou resultados competitivos em vários conjuntos de dados de visão computacional, incluindo o ImageNet, enquanto apresenta uma vantagem significativa em termos de eficiência em relação a outros modelos baseados em Transformer.\n```\nlearn3 = vision_learner(dls, 'levit_256', metrics=error_rate).to_fp16()\nlearn3.fine_tune(8)\n\ninterp = ClassificationInterpretation.from_learner(learn3)\ninterp.plot_confusion_matrix()\n```\n![imagem](https://i.ibb.co/M2PrQyf/matriz-Copia-2.png)\n\n\n## Conclusão\nEm resumo, os modelos ResNet, RegNetY e LeViT têm em comum a mesma finalidade de resolver tarefas de visão computacional, como classificação de imagem. No entanto, esses modelos apresentam diferenças significativas em sua arquitetura, desempenho e eficiência, tornando-os mais adequados para diferentes tipos de aplicações e tarefas específicas.\nPor exemplo, o ResNet18 é um modelo mais simples e menor que o ResNet original, o que o torna mais rápido para treinar e mais leve para ser executado em dispositivos com recursos limitados. É uma opção popular para tarefas de classificação de imagem devido à sua eficácia e eficiência.\nPor outro lado, o RegNetY é capaz de atingir resultados de precisão de ponta em conjunto de dados de referência, como o ImageNet, com uma arquitetura mais enxuta e eficiente do que outras arquiteturas populares, como ResNet e EfficientNet. Isso ocorre porque o RegNetY é construído com base em uma pesquisa automática de arquitetura baseada em RegNetX, que permite a determinação do número de blocos e da largura de banda mais adequados.\nPor fim, a LeViT combina o desempenho de arquiteturas baseadas em Transformer com a eficiência de modelos convolucionais, tornando-a uma opção viável para aplicações em tempo real em dispositivos com recursos limitados. Além disso, a LeViT apresenta vantagens significativas em termos de eficiência em relação a outros modelos baseados em Transformer, tornando-a uma opção atraente para aplicativos com restrições de recursos.\nAo escolher um modelo, deve se levar em conta a acurácia e o tempo, avaliando qual fator tem mais peso dentro do objetivo da aplicação a ser feita.\n\n\n\n",
      "metadata": {
        "tags": []
      }
    }
  ]
}