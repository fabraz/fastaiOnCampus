{"metadata":{"colab":{"provenance":[]},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"gpuClass":"standard"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Classificação de avaliação IMDB","metadata":{"id":"iKD4yfO7k5Le"}},{"cell_type":"markdown","source":"## Autor\n\nLucas Gabriel Bezerra - 180125770\n\nGitHub: [lucasgbezerra](https://github.com/lucasgbezerra/)","metadata":{}},{"cell_type":"markdown","source":"## Objetivo\n\nO objetivo desse artigo é treinar um modelo capaz de identificar os textos de reviews de filmes do IMDB e classificá-las de acordo com o sentimento como positiva ou negativa.\n\n## Motivo\n\nA fim de aplicar conhecimentos de NLP uma das formas de utilizá-lo é a partir da classificação textual. Além disso, o uso de reviews em sites prestadores de serviços tem se tornado cada dia mais comum (Play Store, IMDB, Reclame Aqui), classificar o sentimento de reviews e encaminhá-lo para setores específicos da empresa para tratar da questão envolvida na review pode acelerar o processo de comunicação entre empresa e cliente.","metadata":{}},{"cell_type":"markdown","source":"## Instalação de depêndencias","metadata":{}},{"cell_type":"code","source":"! pip install transformers datasets evaluate","metadata":{"id":"DX2LI2_6k5Lb","outputId":"cdc6b3d4-9137-4d39-af55-18dc7a7d6f8f","colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2022-12-14T14:51:19.156894Z","iopub.execute_input":"2022-12-14T14:51:19.157789Z","iopub.status.idle":"2022-12-14T14:51:32.692636Z","shell.execute_reply.started":"2022-12-14T14:51:19.157694Z","shell.execute_reply":"2022-12-14T14:51:32.691281Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.20.1)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.7/site-packages (2.1.0)\nCollecting evaluate\n  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.12.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.7.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.13.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.5.1)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.3.5)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets) (3.0.0)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2022.8.2)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.13)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (5.0.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets) (3.8.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (21.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.2.0)\nRequirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (0.13.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.7.2)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (4.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (4.1.1)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.3.0)\nRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (2.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (6.0.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.12)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.9.24)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.0)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2022.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\nInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"Para realizar o deploy, irei utilizar o *notebook_login* do huggingface para acessar minha conta e poder salvar o modelo.\n\nDessa forma, será feita a instalação do pacote *huggingface_hub* e a utilização do login dele.\n\nAo executar a função *notebook_login()* um prompt aparece com um campo para adicionar um token de acesso disponibilizado pelo huggingface.","metadata":{}},{"cell_type":"code","source":"!pip install huggingface_hub","metadata":{"execution":{"iopub.status.busy":"2022-12-14T14:52:45.328349Z","iopub.execute_input":"2022-12-14T14:52:45.328749Z","iopub.status.idle":"2022-12-14T14:52:54.855618Z","shell.execute_reply.started":"2022-12-14T14:52:45.328707Z","shell.execute_reply":"2022-12-14T14:52:54.854321Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.7/site-packages (0.10.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from huggingface_hub) (2.28.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface_hub) (3.7.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from huggingface_hub) (6.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from huggingface_hub) (4.13.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface_hub) (4.1.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.7/site-packages (from huggingface_hub) (21.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from huggingface_hub) (4.64.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.9->huggingface_hub) (3.0.9)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->huggingface_hub) (3.8.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface_hub) (3.3)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface_hub) (2.1.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface_hub) (2022.9.24)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface_hub) (1.26.12)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"id":"wL_Ptofvk5Li","execution":{"iopub.status.busy":"2022-12-14T14:53:07.929632Z","iopub.execute_input":"2022-12-14T14:53:07.930562Z","iopub.status.idle":"2022-12-14T14:53:08.160647Z","shell.execute_reply.started":"2022-12-14T14:53:07.930524Z","shell.execute_reply":"2022-12-14T14:53:08.159656Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7f25ea8360c458eaeef62f3a9c93641"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Preparando Dataset","metadata":{"id":"Lb1oELCwk5Li"}},{"cell_type":"markdown","source":"A base de dados utilizada esta disponível no [kaggle](https://www.kaggle.com/datasets/columbine/imdb-dataset-sentiment-analysis-in-csv-format).\n\nPossui 3 CSVs, irei utilizar apenas 2 pra simplificar a criação dos datasets, o resultado obtido se mostrou satisfatorio dessa maneira.\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndf_train = pd.read_csv('/kaggle/input/imdb-dataset-sentiment-analysis-in-csv-format/Train.csv')\ndf_valid = pd.read_csv('/kaggle/input/imdb-dataset-sentiment-analysis-in-csv-format/Valid.csv')","metadata":{"execution":{"iopub.status.busy":"2022-12-14T14:53:21.831214Z","iopub.execute_input":"2022-12-14T14:53:21.831724Z","iopub.status.idle":"2022-12-14T14:53:23.034976Z","shell.execute_reply.started":"2022-12-14T14:53:21.831667Z","shell.execute_reply":"2022-12-14T14:53:23.033910Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"O conjunto de dados tem 2 campos: \n\n- `text`: o texto de review do filme.\n- `label`: um valor de 0 para avaliações negativas ou 1 para avaliações positivas","metadata":{}},{"cell_type":"code","source":"df_train","metadata":{"execution":{"iopub.status.busy":"2022-12-14T14:53:35.498537Z","iopub.execute_input":"2022-12-14T14:53:35.498984Z","iopub.status.idle":"2022-12-14T14:53:35.524882Z","shell.execute_reply.started":"2022-12-14T14:53:35.498946Z","shell.execute_reply":"2022-12-14T14:53:35.523798Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                                    text  label\n0      I grew up (b. 1965) watching and loving the Th...      0\n1      When I put this movie in my DVD player, and sa...      0\n2      Why do people who do not know what a particula...      0\n3      Even though I have great interest in Biblical ...      0\n4      Im a die hard Dads Army fan and nothing will e...      1\n...                                                  ...    ...\n39995  \"Western Union\" is something of a forgotten cl...      1\n39996  This movie is an incredible piece of work. It ...      1\n39997  My wife and I watched this movie because we pl...      0\n39998  When I first watched Flatliners, I was amazed....      1\n39999  Why would this film be so good, but only gross...      1\n\n[40000 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I grew up (b. 1965) watching and loving the Th...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>When I put this movie in my DVD player, and sa...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Why do people who do not know what a particula...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Even though I have great interest in Biblical ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Im a die hard Dads Army fan and nothing will e...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>39995</th>\n      <td>\"Western Union\" is something of a forgotten cl...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>39996</th>\n      <td>This movie is an incredible piece of work. It ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>39997</th>\n      <td>My wife and I watched this movie because we pl...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>39998</th>\n      <td>When I first watched Flatliners, I was amazed....</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>39999</th>\n      <td>Why would this film be so good, but only gross...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>40000 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df_train.describe(include='object')","metadata":{"execution":{"iopub.status.busy":"2022-12-14T14:53:39.770918Z","iopub.execute_input":"2022-12-14T14:53:39.771277Z","iopub.status.idle":"2022-12-14T14:53:39.847619Z","shell.execute_reply.started":"2022-12-14T14:53:39.771245Z","shell.execute_reply":"2022-12-14T14:53:39.846739Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                                                     text\ncount                                               40000\nunique                                              39723\ntop     Hilarious, clean, light-hearted, and quote-wor...\nfreq                                                    4","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>40000</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>39723</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>Hilarious, clean, light-hearted, and quote-wor...</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Transformando o dataframe em dataset utilizando a lib *datasets*","metadata":{}},{"cell_type":"code","source":"from datasets import Dataset,DatasetDict\n\nds_train = Dataset.from_pandas(df_train)\nds_train","metadata":{"execution":{"iopub.status.busy":"2022-12-14T14:53:43.392149Z","iopub.execute_input":"2022-12-14T14:53:43.392506Z","iopub.status.idle":"2022-12-14T14:53:44.174709Z","shell.execute_reply.started":"2022-12-14T14:53:43.392476Z","shell.execute_reply":"2022-12-14T14:53:44.173641Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['text', 'label'],\n    num_rows: 40000\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"## Pré-processamento do texto (Tokenizando e Numeralização)","metadata":{"id":"1ELDErRXk5Lk"}},{"cell_type":"markdown","source":"O processo de tokenização é fundamental no NLP. Ele se resume em converter o texto em lista de palavras (podendo ser em caracteres,ou substrings, dependendo da granularidade do modelo utilizado).\n\nApós realizar a tokenização, será obtido um vocabulário de palavras únicas presentes no dataset. Como um modelo de deep learning espera números como inputs essas palavras(tokens) são convertidas em números.\n\nO modelo utilizado nesse artigo será o *distilbert-base-uncased*.\n\nO DistilBERT base model é uma versão do oriunda do BERT base model. Ele não diferencia caixa alta e caixa baixa, é mais rápido e menor que o BERT.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\nmodel_dbu = \"distilbert-base-uncased\"\n\ntoks = AutoTokenizer.from_pretrained(model_dbu)","metadata":{"id":"Jqwv6p-ok5Ll","execution":{"iopub.status.busy":"2022-12-14T14:53:48.530154Z","iopub.execute_input":"2022-12-14T14:53:48.530725Z","iopub.status.idle":"2022-12-14T14:53:51.490067Z","shell.execute_reply.started":"2022-12-14T14:53:48.530666Z","shell.execute_reply":"2022-12-14T14:53:51.489119Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b245a0936f549658d272a444d05f019"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6f038ea0f5446c19fc79cd3f805b754"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ac14bc934664cf8b3790f11a2f30fbf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66c8cdd3224a403b9f8da83737cbbb9f"}},"metadata":{}}]},{"cell_type":"markdown","source":"Função para tokenizar o texto e truncar sequências, se necessário, para que não sejam maiores que o input aceito pelo modelo DistilBERT.","metadata":{"id":"PW0EiSsxk5Ll"}},{"cell_type":"code","source":"def tokenizer_function(examples):\n    return toks(examples[\"text\"], truncation=True)","metadata":{"id":"3BuR2kpvk5Ll","execution":{"iopub.status.busy":"2022-12-14T14:53:53.911383Z","iopub.execute_input":"2022-12-14T14:53:53.911765Z","iopub.status.idle":"2022-12-14T14:53:53.917004Z","shell.execute_reply.started":"2022-12-14T14:53:53.911731Z","shell.execute_reply":"2022-12-14T14:53:53.915586Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"Aplicando a função a todos os elementos do dataset com a função map.","metadata":{"id":"_4FvYkBlk5Lm"}},{"cell_type":"code","source":"tokenized_imdb = ds_train.map(tokenizer_function, batched=True)","metadata":{"id":"nnbBAfdbk5Lm","execution":{"iopub.status.busy":"2022-12-14T14:53:57.120533Z","iopub.execute_input":"2022-12-14T14:53:57.120926Z","iopub.status.idle":"2022-12-14T14:54:26.710853Z","shell.execute_reply.started":"2022-12-14T14:53:57.120894Z","shell.execute_reply":"2022-12-14T14:54:26.709713Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/40 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c924ecf5e1254419b4ae218d38db6b7e"}},"metadata":{}}]},{"cell_type":"markdown","source":"### Exemplo de tokenização e Numeralização\n\nSegue um exemplo de como o *tokenize()* separa o texto em \"tokens\" e como cada um recebe um número no vocabulário.","metadata":{}},{"cell_type":"code","source":"toks.tokenize('This movie is an incredible piece of work')","metadata":{"execution":{"iopub.status.busy":"2022-12-14T14:54:30.397274Z","iopub.execute_input":"2022-12-14T14:54:30.398384Z","iopub.status.idle":"2022-12-14T14:54:30.406617Z","shell.execute_reply.started":"2022-12-14T14:54:30.398342Z","shell.execute_reply":"2022-12-14T14:54:30.405723Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"['this', 'movie', 'is', 'an', 'incredible', 'piece', 'of', 'work']"},"metadata":{}}]},{"cell_type":"code","source":"toks.vocab['movie']","metadata":{"execution":{"iopub.status.busy":"2022-12-14T14:54:33.230510Z","iopub.execute_input":"2022-12-14T14:54:33.230885Z","iopub.status.idle":"2022-12-14T14:54:33.252845Z","shell.execute_reply.started":"2022-12-14T14:54:33.230855Z","shell.execute_reply":"2022-12-14T14:54:33.251551Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"3185"},"metadata":{}}]},{"cell_type":"markdown","source":"Agora observe um dos textos \"tokenizados\" do dataset utilizado","metadata":{}},{"cell_type":"code","source":"row = tokenized_imdb[0]\nrow['text'], row['input_ids']","metadata":{"execution":{"iopub.status.busy":"2022-12-14T14:54:36.162808Z","iopub.execute_input":"2022-12-14T14:54:36.163164Z","iopub.status.idle":"2022-12-14T14:54:36.174602Z","shell.execute_reply.started":"2022-12-14T14:54:36.163134Z","shell.execute_reply":"2022-12-14T14:54:36.173685Z"},"_kg_hide-input":false,"_kg_hide-output":false,"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"('I grew up (b. 1965) watching and loving the Thunderbirds. All my mates at school watched. We played \"Thunderbirds\" before school, during lunch and after school. We all wanted to be Virgil or Scott. No one wanted to be Alan. Counting down from 5 became an art form. I took my children to see the movie hoping they would get a glimpse of what I loved as a child. How bitterly disappointing. The only high point was the snappy theme tune. Not that it could compare with the original score of the Thunderbirds. Thankfully early Saturday mornings one television channel still plays reruns of the series Gerry Anderson and his wife created. Jonatha Frakes should hand in his directors chair, his version was completely hopeless. A waste of film. Utter rubbish. A CGI remake may be acceptable but replacing marionettes with Homo sapiens subsp. sapiens was a huge error of judgment.',\n [101,\n  1045,\n  3473,\n  2039,\n  1006,\n  1038,\n  1012,\n  3551,\n  1007,\n  3666,\n  1998,\n  8295,\n  1996,\n  8505,\n  12887,\n  1012,\n  2035,\n  2026,\n  14711,\n  2012,\n  2082,\n  3427,\n  1012,\n  2057,\n  2209,\n  1000,\n  8505,\n  12887,\n  1000,\n  2077,\n  2082,\n  1010,\n  2076,\n  6265,\n  1998,\n  2044,\n  2082,\n  1012,\n  2057,\n  2035,\n  2359,\n  2000,\n  2022,\n  17270,\n  2030,\n  3660,\n  1012,\n  2053,\n  2028,\n  2359,\n  2000,\n  2022,\n  5070,\n  1012,\n  10320,\n  2091,\n  2013,\n  1019,\n  2150,\n  2019,\n  2396,\n  2433,\n  1012,\n  1045,\n  2165,\n  2026,\n  2336,\n  2000,\n  2156,\n  1996,\n  3185,\n  5327,\n  2027,\n  2052,\n  2131,\n  1037,\n  12185,\n  1997,\n  2054,\n  1045,\n  3866,\n  2004,\n  1037,\n  2775,\n  1012,\n  2129,\n  19248,\n  15640,\n  1012,\n  1996,\n  2069,\n  2152,\n  2391,\n  2001,\n  1996,\n  10245,\n  7685,\n  4323,\n  8694,\n  1012,\n  2025,\n  2008,\n  2009,\n  2071,\n  12826,\n  2007,\n  1996,\n  2434,\n  3556,\n  1997,\n  1996,\n  8505,\n  12887,\n  1012,\n  16047,\n  2220,\n  5095,\n  16956,\n  2028,\n  2547,\n  3149,\n  2145,\n  3248,\n  2128,\n  15532,\n  2015,\n  1997,\n  1996,\n  2186,\n  14926,\n  5143,\n  1998,\n  2010,\n  2564,\n  2580,\n  1012,\n  6285,\n  8988,\n  2050,\n  25312,\n  9681,\n  2323,\n  2192,\n  1999,\n  2010,\n  5501,\n  3242,\n  1010,\n  2010,\n  2544,\n  2001,\n  3294,\n  20625,\n  1012,\n  1037,\n  5949,\n  1997,\n  2143,\n  1012,\n  14395,\n  29132,\n  1012,\n  1037,\n  1039,\n  5856,\n  12661,\n  2089,\n  2022,\n  11701,\n  2021,\n  6419,\n  10115,\n  26592,\n  2007,\n  24004,\n  20066,\n  24836,\n  24807,\n  1012,\n  20066,\n  24836,\n  2001,\n  1037,\n  4121,\n  7561,\n  1997,\n  8689,\n  1012,\n  102])"},"metadata":{}}]},{"cell_type":"markdown","source":"Agora será criado um lote de exemplos usando o *DataCollatorWithPadding()*, esse lote será utilizado posteriormente como um dos parametrôs do treinamento do modelo.","metadata":{"id":"FqFeIz8Nk5Lm"}},{"cell_type":"code","source":"from transformers import DataCollatorWithPadding\n\ndata_collator = DataCollatorWithPadding(tokenizer=toks)","metadata":{"id":"TRokgHd4k5Lm","execution":{"iopub.status.busy":"2022-12-14T14:55:20.559102Z","iopub.execute_input":"2022-12-14T14:55:20.559530Z","iopub.status.idle":"2022-12-14T14:55:27.700524Z","shell.execute_reply.started":"2022-12-14T14:55:20.559488Z","shell.execute_reply":"2022-12-14T14:55:27.699252Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### Separando o dataset em teste e treino e validação\n\nPara realização do treinamento e testar o modelo após o dataset será dividido em 2 partes, uma contendo 20% dos dados (dados de teste) e a outra com os 80% restantes.\n\nPra isso foi utilizado o método *train_test_split()*.","metadata":{}},{"cell_type":"code","source":"dds = tokenized_imdb.train_test_split(0.20, seed=42)\ndds","metadata":{"execution":{"iopub.status.busy":"2022-12-14T14:55:41.034783Z","iopub.execute_input":"2022-12-14T14:55:41.035191Z","iopub.status.idle":"2022-12-14T14:55:41.058062Z","shell.execute_reply.started":"2022-12-14T14:55:41.035149Z","shell.execute_reply":"2022-12-14T14:55:41.056915Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 32000\n    })\n    test: Dataset({\n        features: ['text', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 8000\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"Realizando a limpeza das labels do dataframe de validação, e convertendo em dataset para utilização posterior ao treinamento do modelo.\n\nComo pode ser visto o dataset de validação é criado a partir de outro dataframe com dados diferentes do dataframe que gerou o dataset de treino e teste","metadata":{}},{"cell_type":"code","source":"df_valid.drop(['label'], axis=1, inplace=True)\nds_valid = Dataset.from_pandas(df_valid)","metadata":{"execution":{"iopub.status.busy":"2022-12-14T14:55:44.385567Z","iopub.execute_input":"2022-12-14T14:55:44.386817Z","iopub.status.idle":"2022-12-14T14:55:44.402898Z","shell.execute_reply.started":"2022-12-14T14:55:44.386766Z","shell.execute_reply":"2022-12-14T14:55:44.401958Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"Realizar a separação dos datasets tem um motivo importantissimo.\n\nIsso evita que ocorra o **overfitting** e o **underfitting**\n\nUma breve explicação:\n\nUm modelo é basicamente um sistema para mapear entradas para saídas. Em suma, um modelo aprende relacionamento entre as entradas (features) e as saídas (labels) a partir de um dataset de treinamento. Depois de treinado o modelo deve ser capaz de receber apenas as entradas (dataset de testes) e ele faz previsões sobre as saídas a partir do que aprendeu no treinamento.\n\nO Modelo mais simples é uma regressão linear e para aumentar a complexidade desse modelo basta aumentar o grau polinomial.\n\n$$\n   y = \\beta_0x + \\beta_1 x² = + \\beta_2 x² + ... + \\beta_n x^n + \\epsilon\n$$\n\n![](https://miro.medium.com/max/720/1*pjIp920-MZdS_3fLVhf-Dw.webp)\n\nO grau do polinomial é a representação da flexibilida do modelo. E a partir disso podemos entender o que é o undefit e o overfit de um modelo.\n\nUm modelo underfit será menos flexível, o que o impede de contabilizar os dados como visto na imagem a seguir.\n\n\n|||\n|--|--|\n|![](https://miro.medium.com/max/640/1*kZfqaD6hl9iYGYXkMwV-JA.webp)| ![](https://miro.medium.com/max/640/1*2RXJ2O-_c2ukaq5p-WQ9tQ.webp)\n\nA função do modelo na cor laranja, na imagem à esquerda, está acima da função real e do conjunto de dados de treinamento. À direita está a previsão dada pelo modelo. Basicamente o modelo ignora o conjunto de treinamento, pois o modelo underfit tem uma **baixa varância**(depende pouco dos dados de treinamento) e **alto viés** (faz uma forte suposição sobre o comportamento dos dados). Quando o modelo tenta fazer previsões o alto viés o faz ter previsões com alta imprecissão.\n\n|||\n|--|--|\n|![](https://miro.medium.com/max/640/1*Di7rY6ALXtkhlmlcKRSCoA.webp)| ![](https://miro.medium.com/max/640/1*QzA45ATjeEbwv5f1G99GnQ.webp)\n\nQuando o grau do modelo é aumentado muito, o modelo consegue alcançar todas alterações dos dados de treinamento, essa alta flexibilidade  faz com que o modelo tenha uma ótima precisão nos dados de treinamento, mas os dados possuem ruídos e essa **alta flexibilidade** faz com que o modelo se ajuste a eles. Assim o modelo irá possuir uma **alta variância**, ele basicamente **memoriza os dados de treinamento** e não \"aprende\" como é o desejado.","metadata":{}},{"cell_type":"markdown","source":"## Avaliação\n\nPara avaliar o desempenho do modelo é preciso ter uma métrica. \n\nPara simplificar a obtenção de um método que possa fazer essa avaliação do modelo, utilizaremos a biblioteca evaluate que possui duzias de métodos de avaliação pra diferentes domínios além do NLP.\n\nA métrica que decide por utilizar foi a métrica de *accuracy**","metadata":{"id":"aBdCJc1lk5Lm"}},{"cell_type":"code","source":"import evaluate\n\naccuracy = evaluate.load(\"accuracy\")","metadata":{"id":"3cGHVK3gk5Ln","execution":{"iopub.status.busy":"2022-12-14T14:59:39.806093Z","iopub.execute_input":"2022-12-14T14:59:39.806672Z","iopub.status.idle":"2022-12-14T14:59:43.047462Z","shell.execute_reply.started":"2022-12-14T14:59:39.806621Z","shell.execute_reply":"2022-12-14T14:59:43.046484Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee506297f0a048eabe68ab4548c0956d"}},"metadata":{}}]},{"cell_type":"markdown","source":"Essa função utiliza da previsão feita e da label para calcular a métrica utilizada (acurácia).","metadata":{"id":"za4jT4OEk5Ln"}},{"cell_type":"code","source":"import numpy as np\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    return accuracy.compute(predictions=predictions, references=labels)","metadata":{"execution":{"iopub.status.busy":"2022-12-14T14:59:54.318129Z","iopub.execute_input":"2022-12-14T14:59:54.318537Z","iopub.status.idle":"2022-12-14T14:59:54.323931Z","shell.execute_reply.started":"2022-12-14T14:59:54.318503Z","shell.execute_reply":"2022-12-14T14:59:54.322947Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"## Treinamento","metadata":{"id":"cUxXN1oak5Ln"}},{"cell_type":"markdown","source":"Map para converter as labes em palavras significantes e o contrário","metadata":{"id":"YXYueR0Nk5Lo"}},{"cell_type":"code","source":"id_label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\nlabel_id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}","metadata":{"execution":{"iopub.status.busy":"2022-12-14T14:59:57.005599Z","iopub.execute_input":"2022-12-14T14:59:57.006319Z","iopub.status.idle":"2022-12-14T14:59:57.010936Z","shell.execute_reply.started":"2022-12-14T14:59:57.006283Z","shell.execute_reply":"2022-12-14T14:59:57.009978Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"Carregando o modelo pré treinado DistilBERT com o número de labels e o mapeamento delas","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_dbu, \n    num_labels=2, \n    id2label=id_label, \n    label2id=label_id\n)","metadata":{"id":"YzmNIb_Uk5Lo","execution":{"iopub.status.busy":"2022-12-14T15:00:21.412481Z","iopub.execute_input":"2022-12-14T15:00:21.413137Z","iopub.status.idle":"2022-12-14T15:00:22.604658Z","shell.execute_reply.started":"2022-12-14T15:00:21.413095Z","shell.execute_reply":"2022-12-14T15:00:22.603665Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Ao executar o *TrainingArguments* uma key para obter automaticamente os pesos e vies era pedido. Não consegui obter a Key então para desabilitei o registro automático de pesos e vieses.","metadata":{}},{"cell_type":"code","source":"import os\n\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"execution":{"iopub.status.busy":"2022-12-14T15:00:40.479035Z","iopub.execute_input":"2022-12-14T15:00:40.479635Z","iopub.status.idle":"2022-12-14T15:00:40.485263Z","shell.execute_reply.started":"2022-12-14T15:00:40.479598Z","shell.execute_reply":"2022-12-14T15:00:40.484092Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"Instalando as depenências de git para ser possível fazer o push do arquivo do modelo para o HugginFace","metadata":{}},{"cell_type":"code","source":"!sudo apt-get install git-lfs\n!git lfs install","metadata":{"execution":{"iopub.status.busy":"2022-12-14T15:03:48.797783Z","iopub.execute_input":"2022-12-14T15:03:48.798906Z","iopub.status.idle":"2022-12-14T15:03:57.040205Z","shell.execute_reply.started":"2022-12-14T15:03:48.798849Z","shell.execute_reply":"2022-12-14T15:03:57.038890Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nThe following NEW packages will be installed:\n  git-lfs\n0 upgraded, 1 newly installed, 0 to remove and 93 not upgraded.\nNeed to get 3316 kB of archives.\nAfter this operation, 11.1 MB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 git-lfs amd64 2.9.2-1 [3316 kB]\nFetched 3316 kB in 0s (6795 kB/s)\nSelecting previously unselected package git-lfs.\n(Reading database ... 108827 files and directories currently installed.)\nPreparing to unpack .../git-lfs_2.9.2-1_amd64.deb ...\nUnpacking git-lfs (2.9.2-1) ...\nSetting up git-lfs (2.9.2-1) ...\nProcessing triggers for man-db (2.9.1-1) ...\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nError: Failed to call git rev-parse --git-dir: exit status 128 \nGit LFS initialized.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Definindo os **argumentos do treinamento com *TrainingArguments()***\n- *output_dir:* Onde salvar o modelo;\n- *learning_rate:* taxa de aprendizagem inicial;\n- *per_device_train_batch_size:* o tamanho da amostra de treino por GPU/TPU;\n- *per_device_eval_batch_size:* o tamanho da amostra de avaliação por GPU/TPU;\n- *num_train_epochs:* Número de épocas para o treinamento;\n- *weight_decay:* A taxa de decaimento do peso a ser aplicada as camadas;\n- *evaluation_strategy:* A estrátegia de avaliação adotada no treinamento;\n- *save_strategy:* A estratégia do ponto de salvamento a ser adotada durante o treinamento;\n- *load_best_model_at_end:* Carregar ou não o melhor modelo encontrado durante o treinamento (Quando verdadeiro exige que os parâmetros \"evaluation_strategy\" e \"save_strategy\" sejam iguais);\n- *push_to_hub:* \"Push\" do modelo para o Hub, quando ele for salvo (depende do \"output_dir\").","metadata":{}},{"cell_type":"markdown","source":"Conhecendo os parâmetros utilizados para a classe *transofrmers.Trainer*:\n- *model:* O modelo a ser treinado, avaliado ou utilizado para previsão;\n- *args:* Os argumentos para ajustar o treinamento;\n- *train_dataset:* O conjunto de dados que será utilizado para o treinamento,\n- *eval_dataset:* O conjunto de dados utilizado para avaliação;\n- *tokenizer:* O tokenizador usado para pré-processar os dados;\n- *data_collator:* A função utilizada para formar um lote a partir de uma lista de elementos de train_dataset ou eval_dataset;\n- *compute_metrics:* a função que será utilizada paar computar as métricas na avaliaçã.","metadata":{}},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"classification_text_model\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=2,\n    weight_decay=0.01,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    push_to_hub=True,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=dds[\"train\"],\n    eval_dataset=dds[\"test\"],\n    tokenizer=toks,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n","metadata":{"id":"4bTKw4gok5Lo","execution":{"iopub.status.busy":"2022-12-14T15:04:43.694730Z","iopub.execute_input":"2022-12-14T15:04:43.695820Z","iopub.status.idle":"2022-12-14T15:06:12.074531Z","shell.execute_reply.started":"2022-12-14T15:04:43.695773Z","shell.execute_reply":"2022-12-14T15:06:12.073246Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"PyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\nUsing the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stderr","text":"Cloning https://huggingface.co/lucasgbezerra/classification_text_model into local empty directory.\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Download file pytorch_model.bin:   0%|          | 7.44k/255M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d50fa7e26bc45a5a84a49281c84d3b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Download file runs/Dec14_01-07-56_f68c23e4e125/events.out.tfevents.1670980121.f68c23e4e125.23.0: 100%|########…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6eab1ee714ff46028c8fcb6065732556"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Download file runs/Dec14_01-10-26_f68c23e4e125/events.out.tfevents.1670980235.f68c23e4e125.23.2: 100%|########…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb2b69fd974c4d628d3b5860a81f33f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Download file runs/Dec14_01-07-56_f68c23e4e125/1670980121.9676027/events.out.tfevents.1670980121.f68c23e4e125.…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"266b6b052e6d4f2e8b7f339743f43fd3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Download file training_args.bin: 100%|##########| 3.23k/3.23k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3c0b36c6cff40b49b375de5d2fb7543"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Download file runs/Dec14_01-10-26_f68c23e4e125/1670980235.297694/events.out.tfevents.1670980235.f68c23e4e125.2…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1020ee1fc7824556af002bcb615139a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Clean file runs/Dec14_01-07-56_f68c23e4e125/events.out.tfevents.1670980121.f68c23e4e125.23.0:  28%|##8       |…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ecb139607864aa4b4bc0c70c5a7d5a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Clean file runs/Dec14_01-10-26_f68c23e4e125/events.out.tfevents.1670980235.f68c23e4e125.23.2:  19%|#9        |…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98557341e4a64440aad53f8bb381975f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Clean file runs/Dec14_01-07-56_f68c23e4e125/1670980121.9676027/events.out.tfevents.1670980121.f68c23e4e125.23.…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d301b3cafd9c4c209d7d1dc29c894bb0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Clean file training_args.bin:  31%|###       | 1.00k/3.23k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a9099d19a1f44fa9e1152a6e0243fec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Clean file runs/Dec14_01-10-26_f68c23e4e125/1670980235.297694/events.out.tfevents.1670980235.f68c23e4e125.23.3…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98a65ea87942489fbee36c351a3f11bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Clean file pytorch_model.bin:   0%|          | 1.00k/255M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80e0c58ab3314a79bc6341ed2d5dcf36"}},"metadata":{}},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2022-12-14T15:06:23.069145Z","iopub.execute_input":"2022-12-14T15:06:23.069532Z","iopub.status.idle":"2022-12-14T15:38:22.311787Z","shell.execute_reply.started":"2022-12-14T15:06:23.069493Z","shell.execute_reply":"2022-12-14T15:38:22.310747Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 32000\n  Num Epochs = 2\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 32\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2000\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2000/2000 31:52, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.204700</td>\n      <td>0.177547</td>\n      <td>0.932125</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.129100</td>\n      <td>0.201243</td>\n      <td>0.934125</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 8000\n  Batch size = 32\nSaving model checkpoint to classification_text_model/checkpoint-1000\nConfiguration saved in classification_text_model/checkpoint-1000/config.json\nModel weights saved in classification_text_model/checkpoint-1000/pytorch_model.bin\ntokenizer config file saved in classification_text_model/checkpoint-1000/tokenizer_config.json\nSpecial tokens file saved in classification_text_model/checkpoint-1000/special_tokens_map.json\ntokenizer config file saved in classification_text_model/tokenizer_config.json\nSpecial tokens file saved in classification_text_model/special_tokens_map.json\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 8000\n  Batch size = 32\nSaving model checkpoint to classification_text_model/checkpoint-2000\nConfiguration saved in classification_text_model/checkpoint-2000/config.json\nModel weights saved in classification_text_model/checkpoint-2000/pytorch_model.bin\ntokenizer config file saved in classification_text_model/checkpoint-2000/tokenizer_config.json\nSpecial tokens file saved in classification_text_model/checkpoint-2000/special_tokens_map.json\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stderr","text":"tokenizer config file saved in classification_text_model/tokenizer_config.json\nSpecial tokens file saved in classification_text_model/special_tokens_map.json\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stderr","text":"\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from classification_text_model/checkpoint-1000 (score: 0.1775466948747635).\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=2000, training_loss=0.1904059410095215, metrics={'train_runtime': 1919.2026, 'train_samples_per_second': 33.347, 'train_steps_per_second': 1.042, 'total_flos': 8474585270592768.0, 'train_loss': 0.1904059410095215, 'epoch': 2.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"O tempo de treinamento foi longo, apesar de utilizar apenas 2 épocas e realizar o treinamento em uma maquina do kaggle com o uso da GPU T4 X2, o treinamento do modelo demorou cerca de 30 minutos. \n\nAssim como configurado nos argumentos o modelo foi salvo em um [repositório](https://huggingface.co/lucasgbezerra/classification_text_model) com o nome especificado na conta logada no ínicio do artigo.","metadata":{}},{"cell_type":"markdown","source":"## Testando o modelo","metadata":{"id":"rU5-H50yk5Lp"}},{"cell_type":"markdown","source":"Vamos verificar o que o modelo prevê a partir do dataset de testes e validação","metadata":{}},{"cell_type":"code","source":"preds = trainer.predict(dds['test']).predictions.astype(float)\npreds","metadata":{"execution":{"iopub.status.busy":"2022-12-14T15:43:59.608031Z","iopub.execute_input":"2022-12-14T15:43:59.608418Z","iopub.status.idle":"2022-12-14T15:45:20.503239Z","shell.execute_reply.started":"2022-12-14T15:43:59.608387Z","shell.execute_reply":"2022-12-14T15:45:20.502070Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Prediction *****\n  Num examples = 8000\n  Batch size = 32\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"array([[ 2.23709369, -2.29509497],\n       [-1.78306019,  1.47043931],\n       [-1.34510422,  0.93321747],\n       ...,\n       [-2.40362573,  1.9160639 ],\n       [ 0.50562054, -0.47438526],\n       [-2.07757139,  1.67809868]])"},"metadata":{}}]},{"cell_type":"markdown","source":"A previsão do dataset de testes retornou números menores que -1 e maiores que 1. Para melhorar o entendimento sobre os resultados e definir limites vamos utilizar o método clip do numpy para deixar estes números dentre o range 0 à 1, que nos permite visualizar como porcentagem o grau de positivadade ou negatividade das frases avaliadas.","metadata":{}},{"cell_type":"code","source":"preds = np.clip(preds, 0, 1)\npreds","metadata":{"id":"9Ok_lUqsk5Lq","execution":{"iopub.status.busy":"2022-12-14T15:47:12.159609Z","iopub.execute_input":"2022-12-14T15:47:12.160631Z","iopub.status.idle":"2022-12-14T15:47:12.169935Z","shell.execute_reply.started":"2022-12-14T15:47:12.160573Z","shell.execute_reply":"2022-12-14T15:47:12.168756Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"array([[1.        , 0.        ],\n       [0.        , 1.        ],\n       [0.        , 0.93321747],\n       ...,\n       [0.        , 1.        ],\n       [0.50562054, 0.        ],\n       [0.        , 1.        ]])"},"metadata":{}}]},{"cell_type":"markdown","source":"Agora utilizando outro conjunto de dados, os dados de validaçaão\n\nPrimeiro eles serão tokenizados para então poder ocorrer a previsão","metadata":{}},{"cell_type":"code","source":"toks_valid = ds_valid.map(tokenizer_function, batched=True)","metadata":{"execution":{"iopub.status.busy":"2022-12-14T15:47:16.476205Z","iopub.execute_input":"2022-12-14T15:47:16.476559Z","iopub.status.idle":"2022-12-14T15:47:20.219545Z","shell.execute_reply.started":"2022-12-14T15:47:16.476528Z","shell.execute_reply":"2022-12-14T15:47:20.218473Z"},"trusted":true},"execution_count":32,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83214ca756e542d494639436e5bbaea0"}},"metadata":{}}]},{"cell_type":"code","source":"preds2 = trainer.predict(toks_valid).predictions.astype(float)\npreds2","metadata":{"execution":{"iopub.status.busy":"2022-12-14T15:49:24.190297Z","iopub.execute_input":"2022-12-14T15:49:24.191328Z","iopub.status.idle":"2022-12-14T15:50:13.490723Z","shell.execute_reply.started":"2022-12-14T15:49:24.191278Z","shell.execute_reply":"2022-12-14T15:50:13.489719Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stderr","text":"The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Prediction *****\n  Num examples = 5000\n  Batch size = 32\n","output_type":"stream"},{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"array([[ 1.95127702, -1.99396503],\n       [ 2.37554598, -2.5228641 ],\n       [ 2.17845559, -2.19129372],\n       ...,\n       [-1.58796299,  1.19799149],\n       [-2.55518889,  2.01890063],\n       [-2.28080058,  1.7151109 ]])"},"metadata":{}}]},{"cell_type":"markdown","source":"Novamente os resultados serão normalizados entre 0 e 1","metadata":{}},{"cell_type":"code","source":"preds2 = np.clip(preds2, 0, 1)\npreds2","metadata":{"execution":{"iopub.status.busy":"2022-12-14T15:50:50.651621Z","iopub.execute_input":"2022-12-14T15:50:50.652039Z","iopub.status.idle":"2022-12-14T15:50:50.660157Z","shell.execute_reply.started":"2022-12-14T15:50:50.651986Z","shell.execute_reply":"2022-12-14T15:50:50.659167Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"array([[1., 0.],\n       [1., 0.],\n       [1., 0.],\n       ...,\n       [0., 1.],\n       [0., 1.],\n       [0., 1.]])"},"metadata":{}}]},{"cell_type":"markdown","source":"## Resultado do deploy\n\nPara realizar o deploy utilizei o modelo salvo no [HuggingFace](https://huggingface.co/lucasgbezerra/classification_text_model).\n\nE a partir dele gerei um Space com o seguinte app.py:\n\n\n```python\nimport gradio as gr\n\ndescription = \"Classificador de sentimento (Positivo, Negativo) de textos de avaliações de filmes\"\nexamples = [\n#             lista de exemplos disponíveis para usar o modelo  \n        ]\n\ngr.Interface.load(\"models/lucasgbezerra/classification_text_model\",  description=description, examples=examples).launch()\n```\n\nPara acessar o deploy basta [clicar aqui](https://huggingface.co/spaces/lucasgbezerra/classification_text)\n\n[![](https://i.imgur.com/GorVTk3.png)](https://huggingface.co/spaces/lucasgbezerra/classification_text)","metadata":{}},{"cell_type":"markdown","source":"## Conclusão\n\nAplicar um modelo pré treinado para NLP é um desafio maior do que o encontrado nos tópicos anteriores de Vision Computer.\n\nA utilização das libs do HugginFace facilitaram bastante, principalmente a nível de entendimento.\n\nA maior dificuldade, quando comparado a Vision Computer está na preparação dos dados. Dados visuais são muito mais simples de serem separados, e erros mais simples de serem notados no conjunto de dados, quando os dados passam a ser textuais a interpretação deles é mais complexa.\n\nOutro ponto a se destacar é o tempo gasto no treinamento, no modelo NLP o tempo gasto em 2 épocas é muito alto, provavelmente muito disso seja devido a quantidade de dados utilizados, cerca de 40 mil textos.\n\nPor último, vale destacar que quanto mais se aprofunda nos parâmetros para a construção do modelo, maior o grau de conhecimento se torna necessário. Aspectos como o underfitting e o overfitting se tornam básicos para entender como seu modelo se porta e como melhorá-lo. A métrica de avaliação se torna mais complexa, e pode ser de diversos tipos, cada uma com um foco e se encaixando melhor em determinado modelo.","metadata":{}}]}